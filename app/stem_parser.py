#!/usr/bin/env python
"""
–û–¥–Ω–æ—Ä–∞–∑–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –æ—Å–Ω–æ–≤ –∏–∑ HTML-—Ä–∞–∑–º–µ—Ç–∫–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º BeautifulSoup.
–ó–∞–ø—É—Å–∫: python import_stems_bs4.py --html-file dictionary.html
"""

import os
import sys
from typing import Dict, List, Optional, Tuple

from tqdm import tqdm

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ Django –ø—Ä–æ–µ–∫—Ç—É
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
os.environ.setdefault("DJANGO_SETTINGS_MODULE", "app.krl.settings")

import django

django.setup()

from bs4 import BeautifulSoup

from lexicon.models import Pos, Stem, Word


class StemParserBS4:
    """–ü–∞—Ä—Å–µ—Ä HTML —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º BeautifulSoup"""

    SUPPORTED_POS = ["s.", "a.", "v."]  # –¢–æ–ª—å–∫–æ —ç—Ç–∏ —á–∞—Å—Ç–∏ —Ä–µ—á–∏

    @staticmethod
    def parse_html_file(html_file: str) -> List[Dict]:
        """–ü–∞—Ä—Å–∏—Ç —Ñ–∞–π–ª, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ –∑–∞–ø–∏—Å–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–º–∏ —á–∞—Å—Ç—è–º–∏ —Ä–µ—á–∏"""
        with open(html_file, "r", encoding="utf-8") as f:
            soup = BeautifulSoup(f.read(), "html.parser")

        entries = []

        for p in soup.find_all("p"):
            b_tag = p.find("b")
            code_tag = p.find("code")
            i_tag = p.find("i")

            if not all([b_tag, code_tag, i_tag]):
                continue

            lemma_with_pipe = b_tag.get_text(strip=True)
            stems_code = code_tag.get_text(strip=True)
            pos_raw = i_tag.get_text(strip=True)

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —á–∞—Å—Ç—å —Ä–µ—á–∏
            pos_abbr, special_marks = StemParserBS4._parse_pos_and_marks(pos_raw)

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —á–∞—Å—Ç–∏ —Ä–µ—á–∏
            if pos_abbr not in StemParserBS4.SUPPORTED_POS:
                continue

            lemma = lemma_with_pipe.replace("|", "")

            entries.append(
                {
                    "lemma": lemma,
                    "lemma_with_pipe": lemma_with_pipe,
                    "pos_abbr": pos_abbr,
                    "special_marks": special_marks,
                    "stems_code": stems_code,
                }
            )

        return entries

    @staticmethod
    def _parse_pos_and_marks(pos_raw: str) -> Tuple[str, List[str]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —á–∞—Å—Ç—å —Ä–µ—á–∏ –∏ –ø–æ–º–µ—Ç—ã"""
        pos_raw = pos_raw.lower().strip()
        parts = []
        current = ""

        for char in pos_raw:
            if char == ".":
                if current:
                    parts.append(current)
                    current = ""
                parts.append(".")
            elif char == " ":
                if current:
                    parts.append(current)
                    current = ""
            else:
                current += char

        if current:
            parts.append(current)

        special_marks = []
        pos_abbr = ""

        i = 0
        while i < len(parts):
            part = parts[i]

            if part in ["s", "a", "v"]:
                pos_abbr = f"{part}."
            elif part == "pl":
                special_marks.append("pl")
            elif part == "sing":
                special_marks.append("sing")
            elif part == "def":
                special_marks.append("v_def")

            i += 1

        return pos_abbr, special_marks


class StemBuilderSimple:
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Å—Ç—Ä–æ–∏—Ç–µ–ª—å –æ—Å–Ω–æ–≤"""

    @staticmethod
    def create_stems_for_word(word: Word, parsed_data: Dict) -> List[Stem]:
        """–°–æ–∑–¥–∞–µ—Ç –æ—Å–Ω–æ–≤—ã –¥–ª—è —Å–ª–æ–≤–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
        lemma = parsed_data["lemma"]
        pos_abbr = parsed_data["pos_abbr"]
        stem_parts = parsed_data["stem_parts"]
        special_marks = parsed_data["special_marks"]

        stems = []

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–∫–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤ –Ω—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å
        if pos_abbr == "v.":
            stem_count = 8  # 0-7
        else:
            stem_count = 6  # 0-5

        # –°–æ–∑–¥–∞–µ–º –æ—Å–Ω–æ–≤—ã
        for i in range(stem_count):
            form = StemBuilderSimple._get_stem_form(
                lemma, i, stem_parts, pos_abbr, special_marks
            )
            if form:
                stem = StemBuilderSimple._create_stem(word, i, form, special_marks)
                if stem:
                    stems.append(stem)

        return stems

    @staticmethod
    def _get_stem_form(
        lemma: str,
        stem_number: int,
        stem_parts: List[Dict],
        pos_abbr: str,
        special_marks: List[str],
    ) -> Optional[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ñ–æ—Ä–º—É –æ—Å–Ω–æ–≤—ã –ø–æ –Ω–æ–º–µ—Ä—É"""

        # –û—Å–Ω–æ–≤–∞ 0 –≤—Å–µ–≥–¥–∞ –ª–µ–º–º–∞
        if stem_number == 0:
            return lemma

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞–∫–æ–π —á–∞—Å—Ç–∏ —à–∞–±–ª–æ–Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —ç—Ç–∞ –æ—Å–Ω–æ–≤–∞
        part_idx = StemBuilderSimple._map_stem_to_part(
            stem_number, pos_abbr, special_marks, len(stem_parts)
        )

        if part_idx is None or part_idx >= len(stem_parts):
            return None

        part = stem_parts[part_idx]

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ —á–µ—Ä–µ–¥–æ–≤–∞–Ω–∏–µ
        use_alternant = StemBuilderSimple._should_use_alternant(stem_number, part)

        if use_alternant and part.get("alternant"):
            suffix = part["alternant"]
        else:
            suffix = part["suffix"]

        # –°—Ç—Ä–æ–∏–º –ø–æ–ª–Ω—É—é —Ñ–æ—Ä–º—É
        return StemBuilderSimple._build_form(lemma, suffix)

    @staticmethod
    def _map_stem_to_part(
        stem_number: int, pos_abbr: str, special_marks: List[str], parts_count: int
    ) -> Optional[int]:
        """–°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–º–µ—Ä –æ—Å–Ω–æ–≤—ã —Å —á–∞—Å—Ç—å—é —à–∞–±–ª–æ–Ω–∞"""

        if pos_abbr == "v.":
            # –ì–ª–∞–≥–æ–ª—ã
            if "v_def" in special_marks:
                # v.def. - –æ—Å–æ–±–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
                mapping = {1: 0, 3: 1, 4: 2, 5: 3, 6: 4}
            else:
                # –û–±—ã—á–Ω—ã–µ –≥–ª–∞–≥–æ–ª—ã
                mapping = {1: 0, 2: 0, 3: 1, 4: 1, 5: 2, 6: 3, 7: 4}
        else:
            # –ò–º–µ–Ω–Ω—ã–µ
            if "pl" in special_marks:
                mapping = {4: 0, 5: 0}
            elif "sing" in special_marks:
                mapping = {1: 0, 2: 0, 3: 1}
            elif parts_count == 3:
                mapping = {1: 0, 3: 1, 4: 2}
            else:
                mapping = {1: 0, 2: 0, 3: 1, 4: 2, 5: 2}

        return mapping.get(stem_number)

    @staticmethod
    def _should_use_alternant(stem_number: int, part: Dict) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ —á–µ—Ä–µ–¥—É—é—â—É—é—Å—è —Ñ–æ—Ä–º—É"""
        if not part.get("has_alternation"):
            return False
        # –î–ª—è –≥–ª–∞–≥–æ–ª–æ–≤: –æ—Å–Ω–æ–≤—ã 2 –∏ 4 –∏—Å–ø–æ–ª—å–∑—É—é—Ç —á–µ—Ä–µ–¥–æ–≤–∞–Ω–∏–µ
        if stem_number in [2, 4]:
            return True
        # –î–ª—è –∏–º–µ–Ω–Ω—ã—Ö: –æ—Å–Ω–æ–≤—ã 2 –∏ 5 –∏—Å–ø–æ–ª—å–∑—É—é—Ç —á–µ—Ä–µ–¥–æ–≤–∞–Ω–∏–µ
        if stem_number in [2, 5]:
            return True

        return False

    @staticmethod
    def _build_form(lemma: str, suffix: str) -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø–æ–ª–Ω—É—é —Ñ–æ—Ä–º—É –æ—Å–Ω–æ–≤—ã"""
        if not suffix:
            return lemma

        return lemma + suffix

    @staticmethod
    def _create_stem(
        word: Word, number: int, form: str, special_marks: List[str]
    ) -> Optional[Stem]:
        """–°–æ–∑–¥–∞–µ—Ç –æ–±—ä–µ–∫—Ç Stem"""
        stem_type = StemBuilderSimple._get_stem_type(word.pos.abbr, number)
        if not stem_type:
            return None

        special_mark = special_marks[0] if special_marks else ""

        return Stem.objects.create(
            word=word,
            stem_type=stem_type,
            number=number,
            form=form,
            special_mark=special_mark,
        )

    @staticmethod
    def _get_stem_type(pos_abbr: str, number: int) -> Optional[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–∏–ø –æ—Å–Ω–æ–≤—ã"""
        try:
            if pos_abbr == "v.":
                return getattr(Stem.VerbStemType, f"VERB_{number}")
            elif pos_abbr == "s.":
                return getattr(Stem.NounStemType, f"NOUN_{number}")
            elif pos_abbr == "a.":
                return getattr(Stem.AdjStemType, f"ADJ_{number}")
        except AttributeError:
            return None
        return None


class SimpleStemImporter:
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∏–º–ø–æ—Ä—Ç–µ—Ä"""

    def __init__(
        self,
        html_file: str,
        dry_run: bool = False,
        skip_existing: bool = False,
        limit: int = 0,
    ):
        self.html_file = html_file
        self.dry_run = dry_run
        self.skip_existing = skip_existing
        self.limit = limit
        self.stats = {
            "total_entries": 0,
            "processed": 0,
            "imported": 0,
            "skipped": 0,
            "errors": [],
            "words_not_found": [],
            "pos_mismatches": [],
        }

    def run(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –∏–º–ø–æ—Ä—Ç"""
        print(f"üìÑ –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞: {self.html_file}")

        try:
            # –ü–∞—Ä—Å–∏–º HTML
            entries = StemParserBS4.parse_html_file(self.html_file)
            self.stats["total_entries"] = len(entries)

            if self.limit > 0:
                entries = entries[: self.limit]

            print(f"üìã –ù–∞–π–¥–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(entries)}")

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–ø–∏—Å–∏
            for entry in tqdm(entries, desc="–ò–º–ø–æ—Ä—Ç"):
                self._process_entry(entry)

            self._print_stats()
            return True

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            import traceback

            traceback.print_exc()
            return False

    def _process_entry(self, entry: Dict):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω—É –∑–∞–ø–∏—Å—å –¥–ª—è –í–°–ï–• –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤ (–æ–º–æ–Ω–∏–º–æ–≤)"""
        self.stats["processed"] += 1

        lemma = entry["lemma"]
        lemma_orig = entry["lemma_with_pipe"]
        pos_abbr = entry["pos_abbr"]

        # –ò—â–µ–º –í–°–ï —Å–ª–æ–≤–∞ —Å –¥–∞–Ω–Ω–æ–π –ª–µ–º–º–æ–π –∏ —á–∞—Å—Ç—å—é —Ä–µ—á–∏
        words = self._find_words(lemma_orig, pos_abbr)

        if not words:
            # –°–ª–æ–≤–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ
            self.stats["words_not_found"].append(f"{lemma_orig} ({pos_abbr})")
            self.stats["skipped"] += 1
            return

        # –õ–æ–≥–∏—Ä—É–µ–º –µ—Å–ª–∏ –æ–º–æ–Ω–∏–º–æ–≤ –±–æ–ª—å—à–µ –æ–¥–Ω–æ–≥–æ
        if len(words) > 1:
            word_ids = sorted([w.id for w in words])

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ
        success_count = 0
        for word in words:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ —É–∂–µ –µ—Å—Ç—å –æ—Å–Ω–æ–≤—ã
            if self.skip_existing and word.stems.exists():
                continue

            try:
                if not self.dry_run:
                    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –æ—Å–Ω–æ–≤—ã
                    word.stems.all().delete()

                    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ –æ—Å–Ω–æ–≤—ã (–¥–ª—è –≤—Å–µ—Ö –æ–º–æ–Ω–∏–º–æ–≤ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ)
                    stems = StemBuilderSimple.create_stems_for_word(word, entry)

                    if stems:
                        success_count += 1
                    else:
                        self.stats["errors"].append(
                            f"–ù–µ —Å–æ–∑–¥–∞–Ω—ã –æ—Å–Ω–æ–≤—ã: {lemma} (word_id: {word.id})"
                        )
                else:
                    # Dry run - —Å—á–∏—Ç–∞–µ–º —É—Å–ø–µ—à–Ω—ã–º
                    success_count += 1

            except Exception as e:
                self.stats["errors"].append(f"{lemma} (word_id: {word.id}): {str(e)}")

        if success_count > 0:
            self.stats["imported"] += success_count
        else:
            self.stats["skipped"] += 1

    def _find_words(self, lemma: str, pos_abbr: str) -> set:
        """–ò—â–µ—Ç –í–°–ï —Å–ª–æ–≤–∞ –≤ –ë–î –ø–æ –ª–µ–º–º–µ, —á–∞—Å—Ç–∏ —Ä–µ—á–∏ –∏ –≤–∞—Ä–∏–∞–Ω—Ç—É, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç set"""
        try:
            pos = Pos.objects.get(abbr=pos_abbr)
            # –ó–∞–º–µ–Ω—è–µ–º  º (U+02BC) –Ω–∞ ‚Äô (U+2019)
            db_lemma = lemma.replace(" º", "‚Äô")

            # –†–∞–∑–±–∏—Ä–∞–µ–º –ª–µ–º–º—É –Ω–∞ —Å–ª–æ–≤–æ –∏ –≤–∞—Ä–∏–∞–Ω—Ç
            # –ü—Ä–∏–º–µ—Ä: "pi|di√§ II" ‚Üí word="pi|di√§", variant="II"
            word_part = db_lemma.strip()
            variant_part = None

            # –ò—â–µ–º —Ä–∏–º—Å–∫—É—é —Ü–∏—Ñ—Ä—É –≤ –∫–æ–Ω—Ü–µ —Å—Ç—Ä–æ–∫–∏
            import re

            match = re.search(r"\s+([IVXLCDM]+)$", db_lemma)
            if match:
                variant_part = match.group(1)
                word_part = db_lemma[: match.start()].strip()

            # –ï—Å–ª–∏ –Ω–µ—Ç –≤–∞—Ä–∏–∞–Ω—Ç–∞ –≤ –ª–µ–º–º–µ, –∏—â–µ–º –≤—Å–µ —Å–ª–æ–≤–∞ –±–µ–∑ –≤–∞—Ä–∏–∞–Ω—Ç–∞
            if not variant_part:
                words = Word.objects.filter(
                    word=word_part, pos=pos, variant__isnull=True
                )
            else:
                # –ò—â–µ–º —Å–ª–æ–≤–∞ —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º –≤–∞—Ä–∏–∞–Ω—Ç–æ–º
                words = Word.objects.filter(
                    word=word_part, pos=pos, variant=variant_part
                )

            return set(words)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ set –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
        except Pos.DoesNotExist:
            self.stats["errors"].append(f"–ß–∞—Å—Ç—å —Ä–µ—á–∏ '{pos_abbr}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –ë–î")
            return set()

    def _print_stats(self):
        """–í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        print("\n" + "=" * 60)
        print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ò–ú–ü–û–†–¢–ê")
        print("=" * 60)
        print(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –≤ —Ñ–∞–π–ª–µ: {self.stats['total_entries']}")
        print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {self.stats['processed']}")
        print(f"–£—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ: {self.stats['imported']}")
        print(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ: {self.stats['skipped']}")

        if self.stats["words_not_found"]:
            print(f"\n‚ö†Ô∏è  –°–ª–æ–≤–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –ë–î ({len(self.stats['words_not_found'])}):")
            for i, word in enumerate(self.stats["words_not_found"][:20]):
                print(f"  {i+1}. {word}")
            if len(self.stats["words_not_found"]) > 20:
                print(f"  ... –∏ –µ—â–µ {len(self.stats['words_not_found']) - 20}")

        if self.stats["errors"]:
            print(f"\n‚ùå –û—à–∏–±–∫–∏ ({len(self.stats['errors'])}):")
            for i, error in enumerate(self.stats["errors"][:10]):
                print(f"  {i+1}. {error}")
            if len(self.stats["errors"]) > 10:
                print(f"  ... –∏ –µ—â–µ {len(self.stats['errors']) - 10}")


def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞"""
    import argparse

    parser = argparse.ArgumentParser(
        description="–ò–º–ø–æ—Ä—Ç –æ—Å–Ω–æ–≤ –∏–∑ HTML —Å BeautifulSoup",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
    –ü—Ä–∏–º–µ—Ä—ã:
    %(prog)s --html-file dictionary.html --dry-run      # –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫
    %(prog)s --html-file dictionary.html --limit 100    # –ü–µ—Ä–≤—ã–µ 100 –∑–∞–ø–∏—Å–µ–π
    %(prog)s --html-file dictionary.html --skip-existing # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ
            """,
    )

    parser.add_argument(
        "--dry-run", action="store_true", help="–¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫ –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è"
    )
    parser.add_argument(
        "--skip-existing", action="store_true", help="–ü—Ä–æ–ø—É—Å–∫–∞—Ç—å —Å–ª–æ–≤–∞ —Å –æ—Å–Ω–æ–≤–∞–º–∏"
    )
    parser.add_argument(
        "--limit", type=int, default=0, help="–û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π"
    )
    parser.add_argument("--output", help="–§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ª–æ–≥–æ–≤")

    args = parser.parse_args()
    html_file = "stems.html"

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–π–ª
    if not os.path.exists(html_file):
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {html_file}")
        sys.exit(1)

    # –ó–∞–ø—É—Å–∫–∞–µ–º –∏–º–ø–æ—Ä—Ç
    importer = SimpleStemImporter(
        html_file=html_file,
        dry_run=args.dry_run,
        skip_existing=args.skip_existing,
        limit=args.limit,
    )

    print("üöÄ –ó–∞–ø—É—Å–∫ –∏–º–ø–æ—Ä—Ç–∞ –æ—Å–Ω–æ–≤...")
    print(f"   –§–∞–π–ª: {html_file}")
    print(f"   Dry run: {'–î–∞' if args.dry_run else '–ù–µ—Ç'}")
    print(f"   –ü—Ä–æ–ø—É—Å–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö: {'–î–∞' if args.skip_existing else '–ù–µ—Ç'}")
    if args.limit > 0:
        print(f"   –õ–∏–º–∏—Ç: {args.limit} –∑–∞–ø–∏—Å–µ–π")

    success = importer.run()

    if success:
        print("\n‚úÖ –ò–º–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
    else:
        print("\n‚ùå –ò–º–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω —Å –æ—à–∏–±–∫–∞–º–∏")

    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
